{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_file_list = glob.glob('data/*FS*')\n",
    "yelp_Reviews_list = glob.glob('data/*Yelp_Reviews*')\n",
    "yelp_file_list = glob.glob('data/*Yelp_0*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': '700 Crossings Blvd',\n",
       " 'cc': 'US',\n",
       " 'city': 'Elverson',\n",
       " 'country': 'United States',\n",
       " 'crossStreet': 'at Morgantown Crossing',\n",
       " 'formattedAddress': ['700 Crossings Blvd (at Morgantown Crossing)',\n",
       "  'Elverson, PA 19520'],\n",
       " 'labeledLatLngs': [{'label': 'display',\n",
       "   'lat': 40.1559562355474,\n",
       "   'lng': -75.8649073541164}],\n",
       " 'lat': 40.1559562355474,\n",
       " 'lng': -75.8649073541164,\n",
       " 'postalCode': '19520',\n",
       " 'state': 'PA'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FS_data_0 = json.load(open(FS_file_list[0], 'r'))\n",
    "FS_data_1 = json.load(open(FS_file_list[1], 'r'))\n",
    "FS_data_2 = json.load(open(FS_file_list[2], 'r'))\n",
    "FS_data_0[0]['response']['venue']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_data_0 = json.load(open(yelp_Reviews_list[0], 'r'))\n",
    "Yelp_data_1 = json.load(open(yelp_Reviews_list[1], 'r'))\n",
    "Yelp_data_2 = json.load(open(yelp_Reviews_list[2], 'r'))\n",
    "Yelp_full = json.load(open(yelp_file_list[2], 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load coreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation,sentiment',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "    \n",
    "    def sentiment(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties={\"annotators\": \"sentiment\"}))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stfSent(text):\n",
    "    a = sNLP.sentiment(text)\n",
    "    sentS = []\n",
    "    for item in a['sentences']:\n",
    "        sentS.append(int(item['sentimentValue']) + 1)\n",
    "    return sentS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create aggregated results from reviews:\n",
    "\n",
    "1. Number of Reviews\n",
    "2. Overall sentiment of the reviews\n",
    "3. For the following categories:\n",
    "    a. Service\n",
    "    b. Food\n",
    "    c. Deal\n",
    "    d. Ambience (environment)\n",
    "    aggregate the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSID2id = {}\n",
    "venueReviews = {}\n",
    "venueTips = {}\n",
    "\n",
    "for idx in range(len(Yelp_data_2)):\n",
    "    try:\n",
    "        venueReviews[idx] = [item['text'] for item in Yelp_data_2[idx]['reviews']]\n",
    "    except Exception:\n",
    "        venueReviews[idx] = []\n",
    "    try:\n",
    "        venueTips[idx] = [item['text'] for item in FS_data_2[idx]['response']['venue']['tips']['groups'][1]['items']]\n",
    "    except Exception:\n",
    "        venueTips[idx] = []\n",
    "    FSID2id[FS_data_0[idx]['response']['venue']['id']] = idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggReviews = {}\n",
    "# aspects = {\n",
    "#     'service': ['serve', 'service', 'server', 'serving', 'order'],\n",
    "#     'food': ['food', 'meal'],\n",
    "#     'deal': ['deal', 'worth', 'price', 'cheap', 'expensive', 'cost'],\n",
    "#     'ambience': ['clean', 'dirty', 'place']\n",
    "# }\n",
    "# for i in range(len(venueReviews)):\n",
    "#     aggReviews[i] = {}\n",
    "#     aggReviews[i]['num_reviews'] = len(venueReviews[i]) + len(venueTips[i])\n",
    "#     aggReviews[i]['num_FS'] = len(venueTips[i])\n",
    "#     aggReviews[i]['num_Yelp'] = len(venueReviews[i])\n",
    "#     _aggReviews = ' '.join(venueReviews[i] + venueTips[i])\n",
    "#     _sents = _aggReviews.split('. ')\n",
    "    \n",
    "#     aggReviews[i]['overall'] = np.mean(stfSent(' '.join(venueReviews[1] + venueTips[1])))\n",
    "#     aggReviews[i]['aspects'] = {}\n",
    "#     for _cat in aspects:\n",
    "#         _tempScore = []\n",
    "#         for _keyWord in aspects[_cat]:\n",
    "#             for _sent in _sents:\n",
    "#                 if _keyWord in _sent:\n",
    "#                     _tempScore.append(np.mean(stfSent(_sent)))\n",
    "#                     continue\n",
    "#         if len(_tempScore) > 0:\n",
    "#             aggReviews[i]['aspects'][_cat] = np.mean(_tempScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aspects': {'food': 2.6875, 'service': 3.0},\n",
       " 'num_FS': 8,\n",
       " 'num_Yelp': 3,\n",
       " 'num_reviews': 11,\n",
       " 'overall': 2.5}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggReviews[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(aggReviews, open('out/aggregated_reviews.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iExit\n",
    "iExit_path = 'data/iExit.csv'\n",
    "\n",
    "lmt = 100\n",
    "i = 0\n",
    "\n",
    "iExit_out = []\n",
    "\n",
    "with open(iExit_path, 'r') as f:\n",
    "    \n",
    "    csvreader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    next(csvreader)\n",
    "    for line in csvreader:\n",
    "        _FSID = line[10]\n",
    "        _orig_order = line[0]\n",
    "        _iExitID = line[1]\n",
    "        _phone = line[3]\n",
    "        _address = line[6]\n",
    "        try:\n",
    "            _aggReview = aggReviews[FSID2id[_FSID]]\n",
    "        except Exception:\n",
    "            _aggReview = {}\n",
    "        _dic = {\n",
    "            'FSID': _FSID,\n",
    "            'orig_order': _orig_order,\n",
    "            'id': _iExitID,\n",
    "            'phone': _phone,\n",
    "            'address': _address,\n",
    "            'reviews': _aggReview\n",
    "        }\n",
    "        iExit_out.append(_dic)\n",
    "#         print(line[10])\n",
    "        \n",
    "#         i+=1\n",
    "#         if i > lmt:\n",
    "#             break\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(iExit_out, open('out/iExit_agg.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
